{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ry38682\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import importlib\n",
    "from unidecode import unidecode\n",
    "from src import table_parser\n",
    "# from src import image_parser\n",
    "from src import section_parser\n",
    "from src import text_parser\n",
    "importlib.reload(table_parser)\n",
    "importlib.reload(section_parser)\n",
    "importlib.reload(text_parser)\n",
    "import itertools as it\n",
    "import os\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pandas as pd\n",
    "# import fitz\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import tiktoken\n",
    "# import pypdfium2 as pdfium\n",
    "from io import BytesIO\n",
    "# import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# from unstructured.partition.auto import partition\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "# from unstructured.partition.image import partition_image\n",
    "from unstructured.cleaners.core import clean,clean_non_ascii_chars,group_broken_paragraphs\n",
    "from langchain.schema.document import Document\n",
    "from tqdm import tqdm\n",
    "# from unstructured.partition.auto import partition_image\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.cleaners.core import clean,clean_non_ascii_chars,group_broken_paragraphs\n",
    "import nltk\n",
    "nltk.download('punkt')  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import ast\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import os.path\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# custom_text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 700,\n",
    "#     chunk_overlap  = 0,\n",
    "#     length_function = len,\n",
    "#     separators = ['. ']\n",
    "# )\n",
    "# def chunker(sample):\n",
    "#     custom_texts = custom_text_splitter.create_documents([sample])\n",
    "#     custom_texts_clean = []\n",
    "#     for custom_text in custom_texts:\n",
    "#         temp = custom_text.page_content.strip('. ')+ \".\"\n",
    "#         custom_texts_clean.append(temp)\n",
    "#     return custom_texts_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open source embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", model_kwargs = {'device': 'cpu'})\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"intfloat/e5-mistral-7b-instruct\", model_kwargs = {'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adobe_table_parser = table_parser.AdobeTableParser()\n",
    "adobe_section_parser = section_parser.AdobeSectionParser()\n",
    "adobe_text_parser = text_parser.AdobeTextParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process Adobe API output file \"structureData.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_rec(section, pos):\n",
    "    result = []\n",
    "    result.append({\n",
    "        \"item\": section[\"section_name\"],\n",
    "        # \"id\": \".\".join([str(x) for x in pos])\n",
    "        \"id\": section[\"id\"]\n",
    "    })\n",
    "    for idx, item in enumerate(section[\"sub_section\"]):\n",
    "        result.extend(parse_json_rec(item, pos+[idx+1]))\n",
    "    return result\n",
    "\n",
    "def create_toc(std_json):\n",
    "    toc = []\n",
    "    for idx, section in enumerate(std_json):\n",
    "        toc.extend(parse_json_rec(section, [idx+1]))\n",
    "\n",
    "    return toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adobe_json = json.load(open(f\"adobe api output/structuredData.json\", encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionName_pageNumber = []\n",
    "for idx in range(7, 45):\n",
    "    toc = adobe_json['elements'][idx]\n",
    "    toc_list = toc['Text'].split('.')\n",
    "    sectionName_pageNumber.append([toc_list[0].strip(), toc_list[0].strip() + '_' + str(toc_list[-1].strip()) + '_' + str(toc_list[-1].strip())])\n",
    "    # header_data['section_name'].append(toc_list[0])\n",
    "    # header_data['page_number'].append(int(float(toc_list[-1].strip())))\n",
    "    # print(toc_list[0], toc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LUCENT INVESTIGATORS (LUCENT Study Group)',\n",
       "  'LUCENT INVESTIGATORS (LUCENT Study Group)_3_3'],\n",
       " ['LUCENT Steering Committee Members',\n",
       "  'LUCENT Steering Committee Members_24_24'],\n",
       " ['SUPPLEMENTAL METHODS', 'SUPPLEMENTAL METHODS_24_24'],\n",
       " ['SELECT INCLUSION CRITERIA: LUCENT-1',\n",
       "  'SELECT INCLUSION CRITERIA: LUCENT-1_24_24'],\n",
       " ['SELECT EXCLUSION CRITERIA: LUCENT-1',\n",
       "  'SELECT EXCLUSION CRITERIA: LUCENT-1_27_27']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectionName_pageNumber[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total objects: 4543\n"
     ]
    }
   ],
   "source": [
    "file_name = 'processed data'\n",
    "\n",
    "adobe_json = json.load(open(f\"adobe api output/structuredData.json\", encoding = 'utf-8'))\n",
    "print(\"Total objects:\", len(adobe_json[\"elements\"]))\n",
    "tabular_data = pd.json_normalize(adobe_json[\"elements\"])\n",
    "table_dict = adobe_table_parser.get_tables_info_dictionary(tabular_data, file_name)\n",
    "section_hierarchy = adobe_section_parser.create_section_hierarchy(adobe_json)\n",
    "if not os.path.exists(f\"outputs/{file_name}/\"): os.makedirs(f\"outputs/{file_name}/\")\n",
    "with open(f\"outputs/{file_name}/intermediate.json\", 'w') as f:\n",
    "    json.dump(section_hierarchy, f, indent = 4)\n",
    "intermediate2 = adobe_text_parser.get_sections_from_json(adobe_json[\"elements\"], section_hierarchy, table_dict)\n",
    "with open(f\"outputs/{file_name}/intermediate2.json\", \"w\") as f:\n",
    "    json.dump(intermediate2, f, indent = 4)\n",
    "\n",
    "toc = create_toc(intermediate2)\n",
    "with open(f\"outputs/{file_name}/intermediate_toc.json\", \"w\") as f:\n",
    "    json.dump(toc, f, indent = 4)\n",
    "\n",
    "final_json = {\n",
    "    \"document_metadata\": {\n",
    "        \"strategy_name\": \"AdobeTextStandardization\",\n",
    "        \"strategy_version\": \"1.0\",\n",
    "        \"file_name\": file_name,\n",
    "        \"document_type\": \"Protocol\",\n",
    "        \"client_name\": \"default\",\n",
    "        \"confidential_flag\": False,\n",
    "    },\n",
    "    \"document_toc\": toc,\n",
    "    \"document_content\": intermediate2\n",
    "}\n",
    "\n",
    "with open(f\"outputs/{file_name}/full_doc_standardized.json\", \"w\") as f:\n",
    "    json.dump(final_json, f, indent = 4)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = json.load(open(\"outputs/processed data/full_doc_standardized.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document_metadata', 'document_toc', 'document_content'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['//Document/Title', '//Document/P', '//Document/P[2]', '//Document/P[3]', '//Document/H1', '//Document/H1[2]', '//Document/H1[3]', '//Document/H1[4]', '//Document/H1[5]', '//Document/H1[6]', '//Document/H1[7]', '//Document/H1[8]'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_hierarchy['parent'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df =table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['//Document/Table', '//Document/Table[2]', '//Document/Table[3]', '//Document/Table[4]', '//Document/Table[5]', '//Document/Table[6]', '//Document/Table[7]', '//Document/Table[8]', '//Document/Table[9]', '//Document/Table[10]', '//Document/Table[11]', '//Document/Table[12]', '//Document/Table[13]', '//Document/Table[14]', '//Document/Table[15]', '//Document/Table[16]', '//Document/Table[17]', '//Document/Table[18]', '//Document/Table[19]'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['section_type', 'text_details', 'image_details', 'table_details', 'start_page', 'end_page', 'id'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_df['//Document/Table'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SUPPLEMENTARY TABLES '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_df['//Document/Table[9]']['table_details']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_chunk_dict = []\n",
    "for key in tabular_df.keys():\n",
    "    text_data = ''\n",
    "    start = tabular_df[key]['start_page']\n",
    "    end = tabular_df[key]['end_page']\n",
    "    header = tabular_df[key]['table_details']['label'].strip()\n",
    "    for dictt in tabular_df[key]['table_details']['body']:\n",
    "        if dictt['col_id'] == 0:\n",
    "            text_data += dictt['value']\n",
    "            # print(dictt['value'])\n",
    "    if text_data != '':\n",
    "        tabular_chunk_dict.append([text_data, header + '_' + str(start)+'_'+str(end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"TEAE, n (%) SAE, n (%) Discontinuation due to AE, n (%) Death, n (%) Common TEAEs, n (%)a Arthralgia Nasopharyngitis Ulcerative colitis Anemia Headache Diarrhoea Injection site pain AEs of interest Infections: All Infections: Serious Infections: Opportunisticb Adjudicated cerebrocardiovascular events Malignanciesc Depressiond Suicide/self-injury Hepatic-related Immediate hypersensitivity reactionse Infusion/injection site reactionsf TEAE=treatment-emergent adverse events; SAE=serious adverse events; AE=adverse events.*One death occurred during the post-treatment follow-up period due to COVID-19 infection after patient discontinued from the study and withdrew consent. a: ≥3% in any treatment group; by decreasing frequency at LUCENT-2 Week 12; b: Once case of gastrointestinal candidiasis and one of cytomegalovirus oesophagitis at Week 12; One case of herpes simplex at Week 40; c: Two Week 12 malignancies were squamous cell carcinoma, one was adenocarcinoma of the colon, and one was rectal cancer; the one Week 40 malignancy was Kaposi's sarcoma; d: excluding suicide or self-injury; e: within 24 hours of drug administration, or on the day of drug administration when time is missing; no serious hypersensitivity or anaphylactic reactions; ; “hypersensitivity reactions” was used as an overarching term to describe systemic events that likely had an allergic or hypersensitivity etiology - analyses for both time periods were based on narrow terms using the following SMQs: anaphylactic reaction, hypersensitivity, and angioedema; f: induction AE of interest is infusion site reaction; maintenance AE of interest is injection site reaction. \", 'a: including cysts and polyps; b: denominator adjusted for gender-specific event for females (induction): PBO N=140, Miri N=367; c: denominator adjusted for gender-specific event for males (induction): PBO N=181, Miri N=591; d: denominator adjusted for gender-specific event for females (maintenance): PBO N=61, Miri N=78;  Table S8: Safety outcomes for open-label extended induction and maintenance period_62.0_62.0']\n"
     ]
    }
   ],
   "source": [
    "print(tabular_chunk_dict[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(tabular_chunk_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding for tabular + Header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "header_tabular_list = sectionName_pageNumber + tabular_chunk_dict\n",
    "print(len(header_tabular_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = []\n",
    "for idx in range(len(header_tabular_list)):\n",
    "    chunk = header_tabular_list[idx][0]\n",
    "    meta_data = header_tabular_list[idx][1]\n",
    "    docs_list.extend([Document(page_content=chunk, metadata={'header_pageNumber':meta_data})]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Chroma.from_documents(docs_list, embeddings, collection_name = 'Chromadb_only_header_table_v4', persist_directory = 'Chromadb_only_header_table_v4/')\n",
    "# db.persist()\n",
    "# print(len(db.get()['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_db = Chroma(collection_name = 'Chromadb_only_header_table_v4', persist_directory=\"Chromadb_only_header_table_v4/\", embedding_function = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'header_pageNumber': 'Table S6: Primary and key secondary outcomes of LUCENT-1 and LUCENT-2 by biologic failure profile_59_59'}\n",
      "\t Table S6: Primary and key secondary outcomes of LUCENT-1 and LUCENT-2 by biologic failure profile \n",
      "\n",
      "{'header_pageNumber': 'Table S1: Primary and major secondary outcomes and definitions for LUCENT-1 and LUCENT-2_52_52'}\n",
      "\t Table S1: Primary and major secondary outcomes and definitions for LUCENT-1 and LUCENT-2 \n",
      "\n",
      "{'header_pageNumber': 'Figure S6: Primary and key secondary outcomes of LUCENT-1 and LUCENT-2 by biologic or tofacitinib failure profile_45_45'}\n",
      "\t Figure S6: Primary and key secondary outcomes of LUCENT-1 and LUCENT-2 by biologic or tofacitinib failure profile \n",
      "\n"
     ]
    }
   ],
   "source": [
    "header_retriever = header_db.as_retriever( search_kwargs ={\n",
    "                                                   \"k\": 3,\n",
    "                                               })\n",
    "\n",
    "# query = \"\"\"\n",
    "# How does Mirikizumab's safety and efficacy vary based on patient's geography?\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "What are all the primary and secondry outcomes of LUCENT1 and LUCENT2\n",
    "\"\"\"\n",
    "\n",
    "# query = \"\"\"\n",
    "# What are baseline demographics and disease characteristics across patient subgroups by geography  \n",
    "# \"\"\"\n",
    "\n",
    "retrieved_docs = header_retriever.get_relevant_documents(query)\n",
    "full_context = ''\n",
    "for doc in retrieved_docs:\n",
    "    full_context += doc.page_content\n",
    "    print(doc.metadata)\n",
    "    print('\\t', doc.page_content, '\\n' )\n",
    "    full_context += '\\n\\n'\n",
    "# print(full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_dict(list_dict):\n",
    "    result = \"\"\n",
    "    current_row_id = None\n",
    "\n",
    "    for item in list_dict:\n",
    "        if current_row_id is None or current_row_id != item['row_id']:\n",
    "            # Change of row_id, add a newline\n",
    "            if result:\n",
    "                result += '\\n'\n",
    "            current_row_id = item['row_id']\n",
    "        else:\n",
    "            # Same row_id, add a separator\n",
    "            result += '--'\n",
    "\n",
    "        # Add the 'value' key to the result\n",
    "        if len(item['value']) > 0:\n",
    "            result += item['value']\n",
    "        else:\n",
    "            result += 'EMPTY'\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_element_to_text(element):\n",
    "#     text = \"\"\n",
    "#     if \"text_details\" in element and \"text\" in element[\"text_details\"]:\n",
    "#         text += str(element[\"text_details\"][\"text\"])\n",
    "#     elif \"image_details\" in element and len(element[\"image_details\"]):\n",
    "#         text+= \"\\nIMAGE HERE\"\n",
    "#     elif \"table_details\" in element and len(element[\"table_details\"]):\n",
    "#         print(element.keys())\n",
    "#         # text+= \"\\nTABLE HERE\"\n",
    "#         text += \"\\n\" + str(format_list_dict(element['table_details']['body']))\n",
    "#     return text\n",
    "\n",
    "# def iterate_rec(sub_json):\n",
    "#     text_list = []\n",
    "#     text_list.append(((sub_json[\"id\"].split(\"|\")[1] + \" \" + sub_json[\"section_name\"]), int(sub_json[\"id\"].split(\"|\")[-1])))\n",
    "#     for element in sub_json[\"section_data\"]:\n",
    "#         text = convert_element_to_text(element)\n",
    "#         text_list.append(('Page_Number:'+str(element['start_page'])+'_'+str(element['end_page'])+ ': '+text, int(element[\"id\"].split(\"|\")[-1])))\n",
    "    \n",
    "#     for element in sub_json[\"sub_section\"]:\n",
    "#         text_list.extend(iterate_rec(element))\n",
    "    \n",
    "#     return text_list\n",
    "\n",
    "# def convert_to_text(full_text_json):\n",
    "#     text = []\n",
    "#     for section in full_text_json[\"document_content\"]:\n",
    "#         text.extend(iterate_rec(section))\n",
    "\n",
    "#     text = sorted(text, key = lambda x: x[-1])\n",
    "#     text = [str(x[0]) for x in text]\n",
    "#     text = \"\\n\".join(text)\n",
    "#     # with open(\"to_text.txt\", \"w\") as f:\n",
    "#     #     f.write(text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "def convert_element_to_text(element):\n",
    "    text = \"\"\n",
    "    if \"text_details\" in element and \"text\" in element[\"text_details\"]:\n",
    "        text += str(element[\"text_details\"][\"text\"])\n",
    "    elif \"image_details\" in element and len(element[\"image_details\"]):\n",
    "        text+= \" \" ##\"\\nIMAGE HERE\"\n",
    "    elif \"table_details\" in element and len(element[\"table_details\"]):\n",
    "        text += \"\\n\" + str(format_list_dict(element['table_details']['body']))\n",
    "    return text\n",
    "\n",
    "def iterate_rec(sub_json):\n",
    "    text_list = []\n",
    "    text_list.append(((sub_json[\"section_name\"]), int(sub_json[\"id\"].split(\"|\")[-1])))\n",
    "    for element in sub_json[\"section_data\"]:\n",
    "        text = convert_element_to_text(element)\n",
    "        text_list.append((text, (element['start_page'], element['end_page']), int(element[\"id\"].split(\"|\")[-1])))\n",
    "    \n",
    "    for element in sub_json[\"sub_section\"]:\n",
    "        text_list.extend(iterate_rec(element))\n",
    "    \n",
    "    return text_list\n",
    "\n",
    "def convert_to_text(full_text_json):\n",
    "    text = []\n",
    "    for section in full_text_json[\"document_content\"]:\n",
    "        text.extend(iterate_rec(section))\n",
    "\n",
    "    # text = sorted(text, key = lambda x: x[-1])\n",
    "    # text = [str(x[0]) for x in text]\n",
    "    # text = \"\\n\".join(text)\n",
    "    # with open(\"to_text.txt\", \"w\") as f:\n",
    "    #     f.write(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_file = convert_to_text(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 1), ('Supplement to: D’Haens G, Dubinsky M, Kobayashi T, et al. Mirikizumab as induction and maintenance therapy for ulcerative colitis. N Engl J Med 2023;388:2444-55. DOI: 10.1056/NEJMoa2207940 \\n\\n', (0, 0), 2), ('This appendix has been provided by the authors to give readers additional information about the work. \\n\\n', (0, 0), 3), ('(PDF updated August 4, 2023) \\n\\n', (0, 0), 4), ('Supplementary Appendix ', 5), ('TABLE OF CONTENTS \\n\\n', (1, 1), 6), ('TABLE OF CONTENTS', 7), ('{}', (1, 2), 8), ('LUCENT INVESTIGATORS (LUCENT Study Group) ', 9), ('Sonja Gassner; Universitätsklinikum Salzburg, Müllner Hauptstraße 50, Salzburg Salzburg 5020, Austria \\n\\n', (3, 3), 10), ('Walter Reinisch; Medical University of Vienna, Waehringer Guertel 18-20, Vienna Vienna 1090, Austria \\n\\n', (3, 3), 11), ('Robert Koch; Medizinische Universität Innsbruck, Anichstraße 35, Innsbruck 6020, Austria \\n\\n', (3, 3), 12), ('Marc Ferrante; Universitaire Ziekenhuizen Leuven - Campus Gasthuisberg, Herestraat 49, Leuven 3000, Belgium \\n\\n', (3, 3), 13), ('Jos Callens; AZ Klina Apotheek – Loskade 1, Augustijnslei 100; Brasschaat 2930, Belgium \\n\\n', (3, 3), 14), ('Pieter Hindryckx; Universitair Ziekenhuis Gent, Corneel Heymanslaan 10, Gent Oost-Vlaanderen 9000, Belgium (2018-2019) \\n\\n', (3, 3), 15), ('Triana Lobaton Ortega; Universitair Ziekenhuis Gent, Corneel Heymanslaan 10, Gent Oost-Vlaanderen 9000, Belgium (2019-close) \\n\\n', (3, 3), 16), ('Souhail Saikali; Centre Hospitalier de Wallonie, picarde - CHwapi A.S.B.L. 9, Avenue Delmée Tournai, Tournai 7500, Belgium \\n\\n', (3, 3), 17), ('Vladimir Borzan; University Hospital Center Osijek, Josipa Huttlera 4, Osijek 31000, Croatia (LUCENT-1 only) \\n\\n', (3, 3), 18), ('Milan Lukas; ISCARE Clinical Centre, Ceskomoravska 2510/19, Praha 9 19000, Czech Republic \\n\\n', (3, 3), 19), ('Miroslava Volfova; Hepato-gastroenterologie HK, s.r.o. Policlinica III., Trida Edvarda Benese 1549, Hradec Kralove 50012, Czech Republic \\n\\n', (3, 3), 20), ('Jiri Pumprla; PreventaMed s.r.o., Domovina 2, Olomouc 779 00, Czech Republic \\n\\n', (3, 3), 21), ('Zdena Zadorova; Fakultni nemocnice Kralovske, Vinohrady Srobarova 50, Dermatovenerologicka klinika Praha 10 Hl. m. Praha 100 34, Czech Republic \\n\\n', (3, 3), 22), ('Stepan Suchanek; Mediendo s.r.o., Thamova 289/13, Praha 8 - Karlin 18600, Czech Republic \\n\\n', (3, 3), 23), ('Lubos Janu; A-Shine s.r.o, Smrkova 23, Plzen 31200, Czech Republic \\n\\n', (3, 3), 24), ('Jan Ulbrych; Fakultni nemocnice u sv. Anny v Brne, Department of Cryostatics Preparation, Pekarska 53, Brno 65691, Czech Republic \\n\\n', (3, 3), 25), ('Jan Gregar; Gregar s.r.o., Cajkovskeho 67/14, Olomouc 779 00, Czech Republic \\n\\n', (3, 3), 26), ('Tomas Brabec; Vojenska nemocnice Brno, Zabrdovicak 3, Brno 636 00, Czech Republic (2018-2020) \\n\\n', (3, 3), 27), ('David Stepek; Vojenska nemocnice Brno, Zabrdovicak 3, Brno 636 00, Czech Republic (2020-close) \\n\\n', (3, 3), 28), ('Signe Wildt; Zealand University Hospital Køge, Section of Gastroenterology, Dept Medicine -Køge, Køge 4600, Denmark \\n\\n', (3, 3), 29), ('Jakob Seidelin; Herlev and Gentofte Hospital, Borgmester Ib Juuls Vej 71, opgang 9, 4. etage, R1, Herlev 2730, Denmark \\n\\n', (3, 3), 30)]\n"
     ]
    }
   ],
   "source": [
    "print(full_text_file[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TABLE OF CONTENTS', 7)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_file[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file = {}\n",
    "for ele in full_text_file:\n",
    "    if len(ele) == 2:\n",
    "        header = ele[0].split('_')[-1]\n",
    "        processed_file[header] = {'text':[], 'page_number':[]}\n",
    "        # print(header)\n",
    "    else:\n",
    "        processed_file[header]['text'].append(ele[0])\n",
    "        processed_file[header]['page_number'].append(ele[1])\n",
    "\n",
    "# for key in processed_file.keys():\n",
    "#     # print(key)\n",
    "#     if len(processed_file[key]['page_number']) > 0:\n",
    "#         page_no = str(processed_file[key]['page_number'][0].split('_')[0]) + '_' + str(processed_file[key]['page_number'][-1].split('_')[-1])\n",
    "#         processed_file[key]['page_number'] = page_no\n",
    "#     else:\n",
    "#         page_no = '-1'\n",
    "#         processed_file[key]['page_number'] = page_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', 'Supplementary Appendix ', 'TABLE OF CONTENTS', 'LUCENT INVESTIGATORS (LUCENT Study Group) ', 'LUCENT Steering Committee Members ', 'SUPPLEMENTAL METHODS ', 'SELECT INCLUSION CRITERIA: LUCENT-1 ', 'Disease-Specific Inclusion Criteria ', 'Prior Medication Failure Criteria ', 'Documentation of dose, frequency, route of administration and duration of the prior failed treatment is required. ', 'Investigators must be able to document an adequate clinical trial of the medication. Patients should fulfill 1 of the following criteria: ', 'UC Medication Dose Stabilization Criteria ', 'SELECT EXCLUSION CRITERIA: LUCENT-1 ', 'Gastrointestinal Exclusion Criteria: ', 'Criteria for Prohibited Medications ', 'SELECT INCLUSION CRITERIA: LUCENT-2 ', 'SELECT EXCLUSION CRITERIA: LUCENT-2 ', 'Gastrointestinal Exclusion Criteria ', 'MAYO SCORE ', 'ENDOSCOPY METHODS ', 'Endoscopic Biopsies ', 'Geboes Scoring ', 'URGENCY NUMERIC RATING SCALE ', 'INFLAMMATORY BOWEL DISEASE QUESTIONNAIRE (IBDQ) ', 'PHARMACOKINETICS ', 'SUMMARY OF ECOA TRANSCRIPTION ERRORS ', 'Rectal Bleeding (RB) electronic clinical outcomes assessment (eCOA) Error in Poland ', 'Stool Frequency eCOA Error in Turkey ', '“How many stools did you have in the past 24 hours?” ', '“How many stools did you have during the night causing you to waken from sleep?” ', 'CORTICOSTEROID TAPERING PROCEDURE ', 'STATISTICAL METHODOLOGY ', 'SUPPLEMENTARY FIGURES ', 'A ', 'B. ', 'SUPPLEMENTARY TABLES ', 'REFERENCES '])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(processed_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_file['LUCENT INVESTIGATORS (LUCENT Study Group) ']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 0), (0, 0)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_file['']['page_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(processed_file['LUCENT INVESTIGATORS (LUCENT Study Group) ']['page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(header, text_list, page_no_list):\n",
    "    chunk_dict = {'chunk': [], 'meta_data': []}\n",
    "    tmp_text = ''\n",
    "    pg_no = []\n",
    "    # print(text_list)\n",
    "    # print(page_no_list)\n",
    "    for idx in range(len(text_list)):\n",
    "        tmp_text += text_list[idx] + ' '\n",
    "        pg_no.append(page_no_list[idx])\n",
    "        if len(tmp_text) > 800:\n",
    "            chunk_dict['chunk'].append(tmp_text)\n",
    "            m_dt = header + '_' + str(pg_no[0][0]) + '_' + str(pg_no[-1][1])\n",
    "            chunk_dict['meta_data'].append(m_dt)\n",
    "            tmp_text = ''\n",
    "            pg_no = []\n",
    "\n",
    "    if len(pg_no) > 0:\n",
    "        chunk_dict['chunk'].append(tmp_text)\n",
    "        m_dt = header + '_' + str(pg_no[0][0]) + '_' + str(pg_no[-1][1])\n",
    "        chunk_dict['meta_data'].append(m_dt)\n",
    "    \n",
    "    return chunk_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunk_dict = {}\n",
    "for header in processed_file.keys():\n",
    "    text_list = processed_file[header]['text']\n",
    "    page_no_list = processed_file[header]['page_number']\n",
    "    chunk_dict = get_chunk(header, text_list, page_no_list)\n",
    "    final_chunk_dict[header] = chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk': ['\\n<>-- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nAll Patients -- <>-- 39 (13.3) -- 210 (24.2) -- 10.9 \\nPooled Age Group 1 -- <>-- <>-- <>-- <>\\n<65 -- <>-- 36 (13.1) -- 199 (25.0) -- 11.9 \\n≥65 -- <>-- 3 (15.8) -- 11 (15.3) -- <>\\nPooled Age Group 2 -- <>-- <>-- <>-- <>\\n<40 -- <>-- 23 (15.5) -- 109 (27.7) -- 12.2 \\n≥40 -- <>-- 16 (11.0) -- 101 (21.3) -- 10.3 \\nSex -- <>-- <>-- <>-- <>\\nMale -- <>-- 20 (12.1) -- 105 (19.8) -- 7.7 \\nFemale -- <>-- 19 (14.7) -- 105 (31.1) -- 16.3 \\nEthnicity -- <>-- <>-- <>-- <>\\nHispanic/Latino -- <>-- 1 (8.3) -- 7 (21.9) -- <>\\nNon-Hispanic/Non-Latino -- <>-- 33 (14.8) -- 155 (24.8) -- 10.0 \\nRace -- <>-- <>-- <>-- <>\\nAmerican Indian/Alaska Native -- <>-- 2 (100.0) -- 2 (20.0) -- <>\\nAsian -- <>-- 8 (11.8) -- 63 (28.3) -- 16.5 \\nBlack/African American -- <>-- 0 (0.0) -- 2 (20.0) -- <>\\nWhite -- <>-- 28 (12.8) -- 142 (23.1) -- 10.3 \\nMultiple -- <>-- 1 (50.0) -- 0 (0.0) -- <>\\nGeographic Region 1 -- <>-- <>-- <>-- <>\\nNorth America -- <>-- 3 (6.4) -- 26 (18.8) -- 12.5 \\nEurope -- <>-- 13 (12.0) -- 69 (22.0) -- 9.9 \\nOther -- <>-- 23 (16.5) -- 115 (27.6) -- 11.1 \\nGeographic Region 2 -- <>-- <>-- <>-- <>\\nAsia -- <>-- 9 (14.3) -- 60 (28.4) -- 14.2 \\nNorth America -- <>-- 3 (6.4) -- 26 (18.8) -- 12.5 \\nCentral America/South America -- <>-- 2 (40.0) -- 4 (22.2) -- <>\\nEast Europe -- <>-- 7 (13.5) -- 38 (26.6) -- 13.1 \\nWest Europe -- <>-- 6 (10.7) -- 31 (18.1) -- 7.4 \\nROW (rest of the world) -- <>-- 12 (16.9) -- 51 (27.3) -- 10.4 \\nBaseline Weight Group 2 -- <>-- <>-- <>-- <>\\n<100 kg -- <>-- 36 (13.0) -- 196 (24.3) -- 11.3 \\n≥100 kg -- <>-- 3 (17.6) -- 14 (22.6) -- <>\\nBaseline BMI Group 1a -- <>-- <>-- <>-- <>\\nUnderweight (<18.5 kg/m2) -- <>-- 5 (17.9) -- 13 (23.6) -- <>\\nNormal (≥18.5 and <25 kg/m2) -- <>-- 19 (12.8) -- 125 (27.7) -- 15.0  ',\n",
       "  '\\n<>-- Risk Difference % (95% CI) -- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nOverweight (≥25 and <30 kg/m2) -- <>-- <>-- 9 (11.8) -- 46 (19.5) -- 7.6 \\nObese (≥30 and <40 kg/m2) -- <>-- <>-- 6 (15.4) -- 22 (19.3) -- 3.9 \\nExtreme obese (≥40 kg/m2) -- <>-- <>-- 0 (0.0) -- 4 (33.3) -- <>\\nTobacco Use -- <>-- <>-- <>-- <>-- <>\\nNever -- <>-- <>-- 30 (15.2) -- 145 (25.0) -- 9.9 \\nCurrent -- <>-- <>-- 2 (11.8) -- 11 (25.6) -- <>\\nFormer -- <>-- <>-- 7 (8.9) -- 54 (22.0) -- 13.1 \\nPrior biologic or tofa exposure -- <>-- <>-- <>-- <>-- <>\\nEver -- <>-- <>-- 12 (9.8) -- 58 (15.4) -- 5.7 \\nNever -- <>-- <>-- 27 (15.8) -- 152 (30.9) -- 15.1 \\nPrior biologic or tofa failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 10 (8.5) -- 55 (15.2) -- 6.8 \\nNot failed -- <>-- <>-- 29 (16.5) -- 155 (30.6) -- 14.1 \\nPrior biologic failureb -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 10 (8.5) -- 55 (15.3) -- 6.7 \\nNot failed -- <>-- <>-- 29 (16.4) -- 155 (30.5) -- 14.1 \\nInadequate response or loss of response to a biologic or tofa -- <>-- <>-- <>-- <>-- <>\\nEver -- <>-- <>-- 10 (8.8) -- 50 (14.6) -- 5.8 \\nNever -- <>-- <>-- 29 (16.1) -- 160 (30.4) -- 14.3 \\nPrior biologic or tofa exposure/failure -- <>-- <>-- <>-- <>-- <>\\nNot exposed -- <>-- <>-- 27 (15.8) -- 152 (30.9) -- 15.1 \\nExposed but not failed -- <>-- <>-- 2 (40.0) -- 3 (20.0) -- <>\\nExposed and failed at least one -- <>-- <>-- 10 (8.5) -- 55 (15.2) -- 6.8 \\nPrior anti-TNF failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 9 (9.3) -- 51 (15.7) -- 6.4 \\nNot failed -- <>-- <>-- 30 (15.2) -- 159 (29.3) -- 14.1 \\nPrior anti-TNF failure and prior failure of either vedo or tofa -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 1 (2.4) -- 16 (11.8) -- 9.3 \\nNot failed -- <>-- <>-- 38 (15.0) -- 194 (26.5) -- 11.5 \\nPrior vedo failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 2 (3.4) -- 18 (11.3) -- 7.9 \\nNot failed -- <>-- <>-- 37 (15.7) -- 192 (27.1) -- 11.3 \\nBaseline corticosteroid use -- <>-- <>-- <>-- <>-- <>\\nYes -- <>-- <>-- 19 (16.8) -- 71 (20.2) -- 3.4 \\nNo -- <>-- <>-- 20 (11.0) -- 139 (26.9) -- 15.8 \\nBaseline immunomodulator use -- <>-- <>-- <>-- <>-- <>\\nYes -- <>-- <>-- 11 (15.9) -- 40 (19.0) -- 3.0 \\nNo -- <>-- <>-- 28 (12.4) -- 170 (25.9) -- 13.4 \\nDuration of UC -- <>-- <>-- <>-- <>-- <>\\n<1 years -- <>-- <>-- 4 (12.1) -- 26 (30.6) -- 18.5 \\n≥1 to <3 years -- <>-- <>-- 13 (16.7) -- 54 (25.6) -- 8.9 \\n≥3 to <7 years -- <>-- <>-- 10 (13.2) -- 58 (24.7) -- 11.5 \\n≥7 years -- <>-- <>-- 12 (11.2) -- 72 (21.4) -- 10.2  ',\n",
       "  '\\nRisk Difference % (95% CI) -- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nBaseline Disease Location -- <>-- <>-- <>-- <>\\nProctitis -- <>-- 0 (0.0) -- 3 (50.0) -- <>\\nLeft-sided colitis -- <>-- 24 (12.8) -- 145 (26.7) -- 13.9 \\nPancolitis -- <>-- 15 (14.6) -- 62 (19.5) -- 4.9 \\nBaseline Fecal Calprotectin -- <>-- <>-- <>-- <>\\n≤250 ug/g -- <>-- 2 (7.1) -- 25 (35.2) -- <>\\n>250 ug/g -- <>-- 29 (12.9) -- 154 (23.1) -- 10.1 \\nBaseline C-reactive Protein (CRP) -- <>-- <>-- <>-- <>\\n≤6 mg/L -- <>-- 24 (13.9) -- 155 (29.5) -- 15.7 \\n>6 mg/L -- <>-- 14 (11.9) -- 53 (16.0) -- 4.1 \\nBaseline Modified Mayo Score -- <>-- <>-- <>-- <>\\nModerate (4-6) -- <>-- 21 (15.2) -- 112 (27.7) -- 12.4 \\nSevere (7-9) -- <>-- 18 (11.6) -- 98 (21.2) -- 9.6 \\n-10 0 -- 10 20 -- 30 -- <>-- <> Abbreviations: Miri = mirikizumab SC Q4W; Pbo = placebo SC Q4W; n = number of patients in the specified category; Diff = Unadjusted risk difference; tofa=tofacitinib; vedo=vedolizumab. Note: Dotted lined represents the estimated risk difference for all patients. \\n\\n '],\n",
       " 'meta_data': ['A _39.0_39.0', 'A _40.0_40.0', 'A _41.0_41']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chunk_dict['A ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Supplementary Appendix \n",
      "TABLE OF CONTENTS\n",
      "LUCENT INVESTIGATORS (LUCENT Study Group) \n",
      "LUCENT Steering Committee Members \n",
      "SUPPLEMENTAL METHODS \n",
      "SELECT INCLUSION CRITERIA: LUCENT-1 \n",
      "Disease-Specific Inclusion Criteria \n",
      "Prior Medication Failure Criteria \n",
      "Documentation of dose, frequency, route of administration and duration of the prior failed treatment is required. \n",
      "Investigators must be able to document an adequate clinical trial of the medication. Patients should fulfill 1 of the following criteria: \n",
      "UC Medication Dose Stabilization Criteria \n",
      "SELECT EXCLUSION CRITERIA: LUCENT-1 \n",
      "Gastrointestinal Exclusion Criteria: \n",
      "Criteria for Prohibited Medications \n",
      "SELECT INCLUSION CRITERIA: LUCENT-2 \n",
      "SELECT EXCLUSION CRITERIA: LUCENT-2 \n",
      "Gastrointestinal Exclusion Criteria \n",
      "MAYO SCORE \n",
      "ENDOSCOPY METHODS \n",
      "Endoscopic Biopsies \n",
      "Geboes Scoring \n",
      "URGENCY NUMERIC RATING SCALE \n",
      "INFLAMMATORY BOWEL DISEASE QUESTIONNAIRE (IBDQ) \n",
      "PHARMACOKINETICS \n",
      "SUMMARY OF ECOA TRANSCRIPTION ERRORS \n",
      "Rectal Bleeding (RB) electronic clinical outcomes assessment (eCOA) Error in Poland \n",
      "Stool Frequency eCOA Error in Turkey \n",
      "“How many stools did you have in the past 24 hours?” \n",
      "“How many stools did you have during the night causing you to waken from sleep?” \n",
      "CORTICOSTEROID TAPERING PROCEDURE \n",
      "STATISTICAL METHODOLOGY \n",
      "SUPPLEMENTARY FIGURES \n",
      "A \n",
      "B. \n",
      "SUPPLEMENTARY TABLES \n",
      "REFERENCES \n"
     ]
    }
   ],
   "source": [
    "for key in final_chunk_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk': ['\\n<>-- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nAll Patients -- <>-- 39 (13.3) -- 210 (24.2) -- 10.9 \\nPooled Age Group 1 -- <>-- <>-- <>-- <>\\n<65 -- <>-- 36 (13.1) -- 199 (25.0) -- 11.9 \\n≥65 -- <>-- 3 (15.8) -- 11 (15.3) -- <>\\nPooled Age Group 2 -- <>-- <>-- <>-- <>\\n<40 -- <>-- 23 (15.5) -- 109 (27.7) -- 12.2 \\n≥40 -- <>-- 16 (11.0) -- 101 (21.3) -- 10.3 \\nSex -- <>-- <>-- <>-- <>\\nMale -- <>-- 20 (12.1) -- 105 (19.8) -- 7.7 \\nFemale -- <>-- 19 (14.7) -- 105 (31.1) -- 16.3 \\nEthnicity -- <>-- <>-- <>-- <>\\nHispanic/Latino -- <>-- 1 (8.3) -- 7 (21.9) -- <>\\nNon-Hispanic/Non-Latino -- <>-- 33 (14.8) -- 155 (24.8) -- 10.0 \\nRace -- <>-- <>-- <>-- <>\\nAmerican Indian/Alaska Native -- <>-- 2 (100.0) -- 2 (20.0) -- <>\\nAsian -- <>-- 8 (11.8) -- 63 (28.3) -- 16.5 \\nBlack/African American -- <>-- 0 (0.0) -- 2 (20.0) -- <>\\nWhite -- <>-- 28 (12.8) -- 142 (23.1) -- 10.3 \\nMultiple -- <>-- 1 (50.0) -- 0 (0.0) -- <>\\nGeographic Region 1 -- <>-- <>-- <>-- <>\\nNorth America -- <>-- 3 (6.4) -- 26 (18.8) -- 12.5 \\nEurope -- <>-- 13 (12.0) -- 69 (22.0) -- 9.9 \\nOther -- <>-- 23 (16.5) -- 115 (27.6) -- 11.1 \\nGeographic Region 2 -- <>-- <>-- <>-- <>\\nAsia -- <>-- 9 (14.3) -- 60 (28.4) -- 14.2 \\nNorth America -- <>-- 3 (6.4) -- 26 (18.8) -- 12.5 \\nCentral America/South America -- <>-- 2 (40.0) -- 4 (22.2) -- <>\\nEast Europe -- <>-- 7 (13.5) -- 38 (26.6) -- 13.1 \\nWest Europe -- <>-- 6 (10.7) -- 31 (18.1) -- 7.4 \\nROW (rest of the world) -- <>-- 12 (16.9) -- 51 (27.3) -- 10.4 \\nBaseline Weight Group 2 -- <>-- <>-- <>-- <>\\n<100 kg -- <>-- 36 (13.0) -- 196 (24.3) -- 11.3 \\n≥100 kg -- <>-- 3 (17.6) -- 14 (22.6) -- <>\\nBaseline BMI Group 1a -- <>-- <>-- <>-- <>\\nUnderweight (<18.5 kg/m2) -- <>-- 5 (17.9) -- 13 (23.6) -- <>\\nNormal (≥18.5 and <25 kg/m2) -- <>-- 19 (12.8) -- 125 (27.7) -- 15.0  ',\n",
       "  '\\n<>-- Risk Difference % (95% CI) -- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nOverweight (≥25 and <30 kg/m2) -- <>-- <>-- 9 (11.8) -- 46 (19.5) -- 7.6 \\nObese (≥30 and <40 kg/m2) -- <>-- <>-- 6 (15.4) -- 22 (19.3) -- 3.9 \\nExtreme obese (≥40 kg/m2) -- <>-- <>-- 0 (0.0) -- 4 (33.3) -- <>\\nTobacco Use -- <>-- <>-- <>-- <>-- <>\\nNever -- <>-- <>-- 30 (15.2) -- 145 (25.0) -- 9.9 \\nCurrent -- <>-- <>-- 2 (11.8) -- 11 (25.6) -- <>\\nFormer -- <>-- <>-- 7 (8.9) -- 54 (22.0) -- 13.1 \\nPrior biologic or tofa exposure -- <>-- <>-- <>-- <>-- <>\\nEver -- <>-- <>-- 12 (9.8) -- 58 (15.4) -- 5.7 \\nNever -- <>-- <>-- 27 (15.8) -- 152 (30.9) -- 15.1 \\nPrior biologic or tofa failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 10 (8.5) -- 55 (15.2) -- 6.8 \\nNot failed -- <>-- <>-- 29 (16.5) -- 155 (30.6) -- 14.1 \\nPrior biologic failureb -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 10 (8.5) -- 55 (15.3) -- 6.7 \\nNot failed -- <>-- <>-- 29 (16.4) -- 155 (30.5) -- 14.1 \\nInadequate response or loss of response to a biologic or tofa -- <>-- <>-- <>-- <>-- <>\\nEver -- <>-- <>-- 10 (8.8) -- 50 (14.6) -- 5.8 \\nNever -- <>-- <>-- 29 (16.1) -- 160 (30.4) -- 14.3 \\nPrior biologic or tofa exposure/failure -- <>-- <>-- <>-- <>-- <>\\nNot exposed -- <>-- <>-- 27 (15.8) -- 152 (30.9) -- 15.1 \\nExposed but not failed -- <>-- <>-- 2 (40.0) -- 3 (20.0) -- <>\\nExposed and failed at least one -- <>-- <>-- 10 (8.5) -- 55 (15.2) -- 6.8 \\nPrior anti-TNF failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 9 (9.3) -- 51 (15.7) -- 6.4 \\nNot failed -- <>-- <>-- 30 (15.2) -- 159 (29.3) -- 14.1 \\nPrior anti-TNF failure and prior failure of either vedo or tofa -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 1 (2.4) -- 16 (11.8) -- 9.3 \\nNot failed -- <>-- <>-- 38 (15.0) -- 194 (26.5) -- 11.5 \\nPrior vedo failure -- <>-- <>-- <>-- <>-- <>\\nFailed -- <>-- <>-- 2 (3.4) -- 18 (11.3) -- 7.9 \\nNot failed -- <>-- <>-- 37 (15.7) -- 192 (27.1) -- 11.3 \\nBaseline corticosteroid use -- <>-- <>-- <>-- <>-- <>\\nYes -- <>-- <>-- 19 (16.8) -- 71 (20.2) -- 3.4 \\nNo -- <>-- <>-- 20 (11.0) -- 139 (26.9) -- 15.8 \\nBaseline immunomodulator use -- <>-- <>-- <>-- <>-- <>\\nYes -- <>-- <>-- 11 (15.9) -- 40 (19.0) -- 3.0 \\nNo -- <>-- <>-- 28 (12.4) -- 170 (25.9) -- 13.4 \\nDuration of UC -- <>-- <>-- <>-- <>-- <>\\n<1 years -- <>-- <>-- 4 (12.1) -- 26 (30.6) -- 18.5 \\n≥1 to <3 years -- <>-- <>-- 13 (16.7) -- 54 (25.6) -- 8.9 \\n≥3 to <7 years -- <>-- <>-- 10 (13.2) -- 58 (24.7) -- 11.5 \\n≥7 years -- <>-- <>-- 12 (11.2) -- 72 (21.4) -- 10.2  ',\n",
       "  '\\nRisk Difference % (95% CI) -- Risk Difference % (95% CI) -- Pbo n (%) -- Miri n (%) -- Diff % \\nBaseline Disease Location -- <>-- <>-- <>-- <>\\nProctitis -- <>-- 0 (0.0) -- 3 (50.0) -- <>\\nLeft-sided colitis -- <>-- 24 (12.8) -- 145 (26.7) -- 13.9 \\nPancolitis -- <>-- 15 (14.6) -- 62 (19.5) -- 4.9 \\nBaseline Fecal Calprotectin -- <>-- <>-- <>-- <>\\n≤250 ug/g -- <>-- 2 (7.1) -- 25 (35.2) -- <>\\n>250 ug/g -- <>-- 29 (12.9) -- 154 (23.1) -- 10.1 \\nBaseline C-reactive Protein (CRP) -- <>-- <>-- <>-- <>\\n≤6 mg/L -- <>-- 24 (13.9) -- 155 (29.5) -- 15.7 \\n>6 mg/L -- <>-- 14 (11.9) -- 53 (16.0) -- 4.1 \\nBaseline Modified Mayo Score -- <>-- <>-- <>-- <>\\nModerate (4-6) -- <>-- 21 (15.2) -- 112 (27.7) -- 12.4 \\nSevere (7-9) -- <>-- 18 (11.6) -- 98 (21.2) -- 9.6 \\n-10 0 -- 10 20 -- 30 -- <>-- <> Abbreviations: Miri = mirikizumab SC Q4W; Pbo = placebo SC Q4W; n = number of patients in the specified category; Diff = Unadjusted risk difference; tofa=tofacitinib; vedo=vedolizumab. Note: Dotted lined represents the estimated risk difference for all patients. \\n\\n '],\n",
       " 'meta_data': ['A _39.0_39.0', 'A _40.0_40.0', 'A _41.0_41']}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chunk_dict['A ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = []\n",
    "for key in final_chunk_dict.keys():\n",
    "    chunk_list = final_chunk_dict[key]['chunk']\n",
    "    meta_data_list = final_chunk_dict[key]['meta_data']\n",
    "    for idx in range(len(chunk_list)):\n",
    "        chunk = chunk_list[idx]\n",
    "        meta_data = meta_data_list[idx]\n",
    "        docs_list.extend([Document(page_content=chunk, metadata={'header_pageNumber':meta_data})]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ChromaDB vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(docs_list, embeddings, collection_name = 'Chromadb_open_src_emb_pdf_v3', persist_directory = 'Chromadb_open_src_emb_pdf_v3/')\n",
    "db.persist()\n",
    "print(len(db.get()['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(collection_name = 'Chromadb_open_src_emb_pdf_v3', persist_directory=\"Chromadb_open_src_emb_pdf_v3/\", embedding_function = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever( search_kwargs ={\n",
    "                                                   \"k\": 5,\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    page_no_list = []\n",
    "    full_context = ''\n",
    "    for doc in retrieved_docs:\n",
    "        full_context += doc.page_content\n",
    "        header, start, end = str(doc.metadata['header_pageNumber']).split('_')\n",
    "        if int(float(start)) == int(float(end)):\n",
    "            page_no_list.append(int(float(start)))\n",
    "        else:\n",
    "            page_no_list.append(int(float(start)))\n",
    "            page_no_list.append(int(float(end)))\n",
    "\n",
    "        print(doc.metadata)\n",
    "        print(len(doc.page_content))\n",
    "        # print('\\t', doc.page_content, '\\n' )\n",
    "        full_context += '\\n\\n'\n",
    "        print('*'*50)\n",
    "    return page_no_list\n",
    "# print(full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'header_pageNumber': 'SUPPLEMENTARY TABLES _57_57.0'}\n",
      "2742\n",
      "**************************************************\n",
      "{'header_pageNumber': 'B. _42.0_42.0'}\n",
      "4608\n",
      "**************************************************\n",
      "{'header_pageNumber': 'A _39.0_39.0'}\n",
      "1794\n",
      "**************************************************\n",
      "{'header_pageNumber': 'B. _41.0_41.0'}\n",
      "1011\n",
      "**************************************************\n",
      "{'header_pageNumber': 'STATISTICAL METHODOLOGY _35_35'}\n",
      "931\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57, 42, 39, 41, 35]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"\"\"\n",
    "# How does Mirikizumab's safety and efficacy vary based on patient's geography?\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# query = \"\"\"\n",
    "# What are all the primary and secondry outcomes of LUCENT1 and LUCENT2\n",
    "# \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "What are baseline demographics and disease characteristics across patient subgroups by geography  \n",
    "\"\"\"\n",
    "get_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_pdf_page(pdf_path, page_number):\n",
    "#     try:\n",
    "#         # Open the PDF file\n",
    "#         pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "#         # Get the specified page\n",
    "#         page = pdf_document[page_number - 1]\n",
    "\n",
    "#         # Display the page\n",
    "#         pdf_document[page_number - 1].show()\n",
    "\n",
    "#         # Close the PDF document\n",
    "#         pdf_document.close()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "def display_pdf_page(pdf_path, page_number):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "        # Get the specified page\n",
    "        page = pdf_document[page_number]\n",
    "\n",
    "        # Render the page as an image\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "        # Display the image using the default image viewer\n",
    "        img.show()\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'nejmoa2207940_appendix.pdf'\n",
    "page_number = 39\n",
    "display_pdf_page(pdf_path, page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_document.ShownPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = json.load(open('structuredData.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.json_normalize(df['elements']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df2[['Page', 'Path', 'Text', 'attributes.ColIndex', 'attributes.ColSpan', 'attributes.NumCol', 'attributes.RowIndex', 'attributes.RowSpan', 'attributes.NumRow']].copy()\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Path</th>\n",
       "      <th>Text</th>\n",
       "      <th>attributes.ColIndex</th>\n",
       "      <th>attributes.ColSpan</th>\n",
       "      <th>attributes.NumCol</th>\n",
       "      <th>attributes.RowIndex</th>\n",
       "      <th>attributes.RowSpan</th>\n",
       "      <th>attributes.NumRow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>//Document/Title</td>\n",
       "      <td>Supplementary Appendix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>//Document/P</td>\n",
       "      <td>Supplement to: D’Haens G, Dubinsky M, Kobayash...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>//Document/P[2]</td>\n",
       "      <td>This appendix has been provided by the authors...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>//Document/P[3]</td>\n",
       "      <td>(PDF updated August 4, 2023)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/H1</td>\n",
       "      <td>Supplementary Appendix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/P[4]</td>\n",
       "      <td>TABLE OF CONTENTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/H1[2]</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/TOC/TOCI/Span</td>\n",
       "      <td>LUCENT INVESTIGATORS (LUCENT Study Group) .......</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/TOC/TOCI[2]/Span</td>\n",
       "      <td>LUCENT Steering Committee Members................</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>//Document/TOC/TOCI[3]/Span</td>\n",
       "      <td>SUPPLEMENTAL METHODS.............................</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page                         Path  \\\n",
       "0   0.0             //Document/Title   \n",
       "1   0.0                 //Document/P   \n",
       "2   0.0              //Document/P[2]   \n",
       "3   0.0              //Document/P[3]   \n",
       "4   1.0                //Document/H1   \n",
       "5   1.0              //Document/P[4]   \n",
       "6   1.0             //Document/H1[2]   \n",
       "7   1.0     //Document/TOC/TOCI/Span   \n",
       "8   1.0  //Document/TOC/TOCI[2]/Span   \n",
       "9   1.0  //Document/TOC/TOCI[3]/Span   \n",
       "\n",
       "                                                Text  attributes.ColIndex  \\\n",
       "0                            Supplementary Appendix                   NaN   \n",
       "1  Supplement to: D’Haens G, Dubinsky M, Kobayash...                  NaN   \n",
       "2  This appendix has been provided by the authors...                  NaN   \n",
       "3                      (PDF updated August 4, 2023)                   NaN   \n",
       "4                            Supplementary Appendix                   NaN   \n",
       "5                                 TABLE OF CONTENTS                   NaN   \n",
       "6                                          Contents                   NaN   \n",
       "7  LUCENT INVESTIGATORS (LUCENT Study Group) .......                  NaN   \n",
       "8  LUCENT Steering Committee Members................                  NaN   \n",
       "9  SUPPLEMENTAL METHODS.............................                  NaN   \n",
       "\n",
       "   attributes.ColSpan  attributes.NumCol  attributes.RowIndex  \\\n",
       "0                 NaN                NaN                  NaN   \n",
       "1                 NaN                NaN                  NaN   \n",
       "2                 NaN                NaN                  NaN   \n",
       "3                 NaN                NaN                  NaN   \n",
       "4                 NaN                NaN                  NaN   \n",
       "5                 NaN                NaN                  NaN   \n",
       "6                 NaN                NaN                  NaN   \n",
       "7                 NaN                NaN                  NaN   \n",
       "8                 NaN                NaN                  NaN   \n",
       "9                 NaN                NaN                  NaN   \n",
       "\n",
       "   attributes.RowSpan  attributes.NumRow  \n",
       "0                 NaN                NaN  \n",
       "1                 NaN                NaN  \n",
       "2                 NaN                NaN  \n",
       "3                 NaN                NaN  \n",
       "4                 NaN                NaN  \n",
       "5                 NaN                NaN  \n",
       "6                 NaN                NaN  \n",
       "7                 NaN                NaN  \n",
       "8                 NaN                NaN  \n",
       "9                 NaN                NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//Document/Title', '//Document/P', '//Document/P[2]', '//Document/P[3]', '//Document/H1', '//Document/P[4]', '//Document/H1[2]', '//Document/TOC/TOCI/Span', '//Document/TOC/TOCI[2]/Span', '//Document/TOC/TOCI[3]/Span', '//Document/TOC/TOCI[4]/Span', '//Document/TOC/TOCI[5]/Span', '//Document/TOC/TOCI[6]/Span', '//Document/TOC/TOCI[7]/Span', '//Document/TOC/TOCI[8]/Span', '//Document/TOC/TOCI[9]/Span', '//Document/TOC/TOCI[10]/Span', '//Document/TOC/TOCI[11]/Span', '//Document/TOC/TOCI[12]/Span', '//Document/TOC/TOCI[13]/Span', '//Document/TOC/TOCI[14]/Span', '//Document/TOC/TOCI[15]/Span', '//Document/TOC/TOCI[16]/Span', '//Document/TOC/TOCI[17]/Span', '//Document/TOC/TOCI[18]/Span', '//Document/TOC/TOCI[19]/Span', '//Document/TOC/TOCI[20]/Span', '//Document/TOC/TOCI[21]/Span', '//Document/TOC/TOCI[22]/Span', '//Document/TOC/TOCI[23]/Span', '//Document/TOC/TOCI[24]/Span', '//Document/TOC/TOCI[25]/Span', '//Document/TOC/TOCI[26]/Span', '//Document/TOC/TOCI[27]/Span', '//Document/TOC/TOCI[28]/Span', '//Document/TOC/TOCI[29]/Span', '//Document/TOC/TOCI[30]/Span', '//Document/TOC/TOCI[31]/Span', '//Document/TOC/TOCI[32]/Span', '//Document/TOC/TOCI[33]/Span', '//Document/TOC/TOCI[34]/Span', '//Document/TOC/TOCI[35]/Span', '//Document/TOC/TOCI[36]/Span', '//Document/TOC/TOCI[37]/Span', '//Document/TOC/TOCI[38]/Span', '//Document/H1[3]', '//Document/P[5]', '//Document/P[6]', '//Document/P[7]', '//Document/P[8]', '//Document/P[9]', '//Document/P[10]', '//Document/P[11]', '//Document/P[12]', '//Document/P[13]', '//Document/P[14]', '//Document/P[15]', '//Document/P[16]', '//Document/P[17]', '//Document/P[18]', '//Document/P[19]', '//Document/P[20]', '//Document/P[21]', '//Document/P[22]', '//Document/P[23]', '//Document/P[24]', '//Document/P[25]', '//Document/P[26]', '//Document/P[27]', '//Document/P[28]', '//Document/P[29]', '//Document/P[30]', '//Document/P[31]', '//Document/P[32]', '//Document/P[33]', '//Document/P[34]', '//Document/P[35]', '//Document/P[36]', '//Document/P[37]', '//Document/P[38]', '//Document/P[39]', '//Document/P[40]', '//Document/P[41]', '//Document/P[42]', '//Document/P[43]', '//Document/P[44]/ParagraphSpan', '//Document/P[44]/ParagraphSpan[2]', '//Document/P[45]', '//Document/P[46]', '//Document/P[47]', '//Document/P[48]', '//Document/P[49]', '//Document/P[50]', '//Document/P[51]', '//Document/P[52]', '//Document/P[53]', '//Document/P[54]', '//Document/P[55]', '//Document/P[56]', '//Document/P[57]']\n"
     ]
    }
   ],
   "source": [
    "print(list(filtered_df['Path'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
